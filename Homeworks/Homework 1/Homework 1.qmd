---
title: "Statistical Learning, Homework #1"
date: 2024-03-27
author: "Jacopo Manenti, id: 247279"
format: 
      pdf: 
        latex_engine: xelatex
editor: visual
---

```{r setup, include = FALSE}
# Package names
packages <- c("tidyverse", "dplyr","gridExtra", "caret","class","ROCR","purrr", "ggpubr","kableExtra")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      tidy.opts=list(width.cutoff = 80),
                      tidy = TRUE)

```

# Introduction and Data Exploration

In this paper, we explore factors influencing pregnant women's decisions regarding breastfeeding their babies. Data from a study at a UK hospital involving 139 participants are stored in "bf.csv". We will conduct logistic regression and k-nearest neighbors analysis to understand factors affecting the decision of pregnant women to breastfeed their babies. This analysis might be useful in targeting breastfeeding promotions effectively.

```{r import_data}
df <- read.csv("bf.csv") # import data 
dim(df) # get dimension
```

In Table 1 it has been reported a glimpse of the data set and in Table 2 the variables' data structure. The **breast** variable is the response variable, categorizing feeding practices into breastfeeding (Breast) and exclusive bottle feeding (Bottle). Since logistic regression model requires a Bernoulli variable, we will encode it into binary values (1 and 0, respectively). Furthermore, most variables are categorical, so to facilitate the modeling process, we will organize them into factors. Additionally, we look at missing values reported in Table 3, which mostly concern age and education level, notably among bottle-feeding and white mothers, indicating possible data collection issues. Thus, we remove these records, mindful of potential impacts on class balance. Finally, Table 4 displays the recoded and cleaned version of the data set, with fewer rows resulting from the removal of NA values (135). Following this data preparation step, we will perform basic statistical analyses and generate relevant plots.

```{r Table_1&Table_2, echo = FALSE}

# function for plotting data set head
table_plot <- function(table, cap , lab, ...) {
  table %>% 
  knitr::kable( caption = cap, format="latex", booktabs = TRUE, align ="c", label = lab)%>%   kableExtra::kable_styling(full_width = FALSE,position = "center", latex_options = c("scale_down","HOLD_position"))
}

# function for creating a table with data types
datatypes_table <- function(data) {
  types <- sapply(data, class)
  types_matrix <- matrix(types, nrow = 1)
  types_df <- as.data.frame(types_matrix, stringsAsFactors = FALSE)
  colnames(types_df) <- names(data)
  return(types_df)
}

glimpse<- head(df,6) # get the first 6 rows of the data set


# Plot table 1
table_plot(glimpse, cap = "shows the first 6 rows in the data set.", lab = "Table 1") 

# plot table 2
table_plot(datatypes_table(df), cap ="shows the data type of each variable of the data set.", lab ="Table 2") 

```

```{r Table_3, echo = FALSE }
df %>% 
  filter(!complete.cases(.)) %>% # get the rows containing NA's values
  table_plot(., cap = "shows the rows containing NA values.", lab = "Table 3") # plot table 3

```

```{r Table_4, echo = FALSE}
df <- df %>% 
    select(names(df)) %>% # select variables' names
  na.omit() %>% # remove rows containing NAs values
  mutate(breast = recode(breast, 
                         Breast = "1",
                         Bottle = "0")) %>% # recode the response variable into binary
  mutate_if(is.character,as.factor) # mutate the structure from character to factors
  
#plot table 4
head(df,4) %>% 
  table_plot(. , cap = "shows the cleaned version of the dataset with 135 rows (observations)", lab = "Table 4")


attach(df) # attach the data set
```

In Table 5 it has been reported the statistical summary of the dataset. It can be clearly seen that the *response variable* has a significant imbalance between classes, with 99 out of 135 instances reporting breastfeeding (gaphical representation at Figure 1). Also, among the 135 participants, only 21 are single, and a mere 32 are current smokers. The data set comprises patients aged between 17 and 40 years, having studed up to the age of 18 on average. Interviews were conducted balancedly across both races.

```{r Table_5, echo = FALSE}
#| label: tbl-summary-stat
#| tbl-cap: "summary statistics of response variable and demographic factors"
#| tbl-subcap: ["breast", "partner", "age", "smokenow", "educat"]
#| layout-ncol: 5

sum_df <- summary(df) # get summary of data set

# plot just relevant observations
kable(summary(df["breast"]))
kable(summary(df["partner"]))
kable(summary(df["age"]))
kable(summary(df["smokenow"]))
kable(summary(df["educat"]))



 
```

```{r imbalance_table, fig.width=5, fig.height=2, fig.cap="Illustrates breastfeeding (blue) vs. bottle-feeding (red) proportions.", fig.align="center", out.width="50%" ,echo = FALSE}

# compute the proportions for 'breast', 'smokenow' and 'partner' variables
prop_breast<- round(prop.table(table(breast)) *100,2)

# converting prop_breast into a dataframe
prop_breast_d <- as.data.frame(prop_breast)

# barplots from the dataframe 
prop_breast_d %>% 
    ggplot(aes(x = breast, y = Freq, fill=breast))+
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    scale_fill_manual(values = c("red", "blue")) +
    labs(x = "breastfeeding methods", 
         y = "Proportion by class", 
         title = "Class Distribution of the Response Variable") + 
    theme(plot.title = element_text(hjust = 0.5)) +
    guides(fill = guide_legend(title = "Breastfeeding Methods"))

```

We proceedexploring the relationship between numerical features (**age** and **educat**) and breastfeeding practices. Figure 2 (A) shows scatterplot, where breastfeeding individuals are in blue, non-breastfeeding in orange. Notably, a pattern emerges suggesting that the first groups tended to have a higher educational attainment compared to the other. This is reinforced by the boxplots, B for age and C for education, as *C* reports a revelant difference between the two conditionated classes (Bottle-feeding distribution is concentrated at lower education levels).

```{r numerical, fig.width=10, fig.height=4, fig.cap="*Top*: Scatterplot (A) of mothers' age and education, red for breastfeeding, blue for non. *Center*: Boxplots of age (B) and education (C) by response status.", fig.align="center", echo = FALSE,  out.width="80%" }

# assign the scatterplot to a variable
numeric_scatter <-  df %>% 
      ggplot(aes(x= age,
                 y= educat,
                 colour= breast))+
      geom_point()+
      labs(title="scatterplo age vs educat")+
      theme_minimal()+
      theme(plot.title = element_text(hjust = 0.5))


# assign the box plots to variables
age_box <- df %>% 
              ggplot(aes(x= breast, y=age, fill=breast))+
              geom_boxplot()+
              theme_classic()

educat_box <- df %>% 
              ggplot(aes(x= breast, y=educat, fill=breast))+
              geom_boxplot()+
              theme_classic()

# plot scatterplot and boxplots into a single frame
ggarrange(numeric_scatter,
          ggarrange(age_box,educat_box, ncol=2, labels= c("B","C")), 
          labels = "A",
          heights = c(4,4),
          nrow = 2)

```

```{r correlation_matr, echo = FALSE}
#| label: tbl-corr-matrx
#| tbl-cap: "Shows correlation matrix between age and educational attainment, revealing no relevant association."

# compute and plot the correlation matrix
df %>%    
  select(!is.factor) %>%      
  cor() %>% 
  kable(.)
```

For categorical features, bar plots aid in similar analysis: consistent disparities in bar heights may suggest an association between predictor and response. In Figure 3, this behavior is seen in **howfdfr**, **smokenow**, **smokebf**, and **ethnic**. We confirm this with a chi-square test; Table 7 shows significance only for these predictors, indicating correlation with baby-feeding methodology.

```{r categorical, fig.width=10, fig.height=4, fig.cap="Bar plots for predictors and response (pink: breastfeeding, red: bottle-feeding).", fig.align="center", echo = FALSE, out.width="80%"}

# get only categorical variables but breast
plot_names <- df %>%
  select_if(is.factor) %>%
  select(-breast)

# initialize a list where we will save graphs
plots_list <- list()

# compute and append each  bar plot to the list 
for (i in names(plot_names)){
  tbl <- table(df[[i]], breast)
  fly <- as.data.frame(tbl)
  colnames(fly) <- c("Method", "breast", "Frequency")
  
  br <- fly %>% 
    ggplot(aes(x = Method, y = Frequency, fill = breast))+
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    labs(x = i, y = paste(i, "counts"), title = paste(i, "and breast")) +
    scale_fill_manual(values = c("red", "pink")) +
    theme_minimal()+
    theme(plot.title = element_text(hjust = 0.5))
  
  plots_list[[i]] <- br
  
}

# plot bar plots in the same plot grid
ggarrange(plotlist = plots_list, ncol = 4, nrow = 2)

```

```{r chi_square, echo =FALSE}
#| label: tbl-chisq
#| tbl-cap: "shows results of chisq-test between pairs of each categorical and breast variable"

# initialize empty vectors where we will save results
all_var <- all_chi_s <- all_p_val <- c()

# loop for computing the chi-square results and append theme to the initialized vectors
for (i in names(plot_names)){
    tbl <- table(plot_names[[i]],breast)
    chi_sq<- chisq.test(tbl)
    
    # all in one dataset
    all_var <- append(all_var, i)
    all_chi_s <- append(all_chi_s, chi_sq$statistic)
    all_p_val <-append(all_p_val,round(chi_sq$p.value, 6))
}

# convert to a data frame
test_df <- data.frame(
  Variable = all_var,
  Chi_squared = all_chi_s,
  P_value = all_p_val
)

#plot the data frame
kable(test_df)

```

# Splitting the data set into training and testing sets

Before proceeding with the classification task, we split the dataset into training (df_train; 100 samples) and testing sets (df_test; 35 samples), maintaining the observed class imbalance.

```{r split_data}
set.seed(5) # Set the seed for reproducible results
# get proportion of class
prop_breast<- round(prop.table(table(breast)) *100,2)
# Obtain indices
train_indices <- createDataPartition(breast,p=prop_breast[["1"]]/100, list = FALSE)  
#create a train set and a vector of its labels
df_train <- df[train_indices,]
y_tr <-  breast[train_indices]
#create a test set and a vector of its labels
df_test  <- df[-train_indices, ]
y_ts <- breast[-train_indices]
```

```{r check_prop, include=FALSE}
#check for training set's class unbalance
prop.table(table(y_tr)) == round(prop_breast/100,2)
# check for test set's class unbalance
prop.table(table(y_ts)) == round(prop_breast/100,2) # close, probably due to small sample
```

# Fit a Logistic Regression model and a k-NN

We can now start modelling a logistic regression (using the training set) for the purpose of estimating the probability of a mother breastfeeding her baby (coded as 1) given specific factor configurations, denoted as $P(breast = 1 | factors)$. We estimate the values of the coefficients by the following approach: $$\begin{aligned}
\text{logit}(E(\text{breast})) = \beta_0 + \beta_1 \text{pregnancy} + \beta_2 \text{howfed} + \beta_3 \text{howfedfr} + \beta_4 \text{partner} \\  + \beta_5 \text{age} + \beta_6 \text{educat} + \beta_7 \text{ethnic} \quad + \beta_8 \text{smokenow} + \beta_9 \text{smokebf}\end{aligned}$$ Table 8 presents the estimated coefficients and associated information of the fitted model. We conclude that, apart from **howfedfr**, **smokenow** and **ethnic**, there is no significant association between the predictors and the probability. The value of each coefficients means a variation in the probability of breastfeeding. For example, the coefficient of **smokenow** is negative, indicating that mothers that smoke are less likely breastfeed than non-smokers. To be more specific, holding other variables constant, smoking is associated with a decrease in the log odds of breastfeeding by 3.51100 units, and subsequently with a decrease in probability.

```{r glm_fit}
mod1 <- glm(breast~., data = df_train, family=binomial) # fit the model
mod1_sum <- summary(mod1) # retrieve associated information
```

```{r glm_summary,echo =FALSE}
#| label: tbl-glm_summary
#| tbl-cap: "shows fitted logistic regression coefficients for predicting breastfeeding likelihood."

#plot fitted coefficients
kable(mod1_sum$coefficients)
```

After fitting the logistic regression, we proceed to the k-nearest neighbors (k-NN) classifier. To determine the optimal value for $k$, we compare values from 1 to 100 to find the one with the lowest error rate, indicating the fraction of misclassified test observations. Choosing the value of $k$ that minimizes this error helps obtain a more accurate model that generalizes more effectively. From Figure 4, the optimal value is $k = 9$.

```{r k_nn, echo = FALSE}
# create train and test set for K-NN
x_train <- df_train %>% 
    select(-breast) %>% 
    mutate_if(is.factor,as.numeric) %>%  # mutate factors into numeric, avoiding coercion warning message with knn()
    mutate(across(where(is.numeric), ~case_when(. == 1 ~ 0, . == 2 ~ 1, TRUE ~ .))) #recoding 
x_test <- df_test %>% 
    select(-breast) %>% 
    mutate_if(is.factor,as.numeric) %>%  # mutate factors into numeric, avoiding coercion warning message with knn()
    mutate(across(where(is.numeric), ~case_when(. == 1 ~ 0, . == 2 ~ 1, TRUE ~ .))) # recoding

# define a function to compute the error rate
calc_error_rate <- function(predicted.value, true.value) {
mean(true.value != predicted.value)
}

# initialize the error rate vector
errors_ts <- c()
# initialize the k values vector
kvec <- c(seq(1:100))
# loop for computing different value of error rate in respect to changing k's values
for (k in kvec) {
  pred_ts <- knn(x_train, x_test, y_tr, k = k)
  err_ts <- calc_error_rate(pred_ts, y_ts)
  errors_ts <- append(errors_ts, err_ts)
}
# select the k value that minimizes the error rate
k_star <- kvec[which.min(errors_ts)]
```

```{r _optimal_k, fig.cap="The KNN test error rate as the number of neighbors K decreases.", fig.align="center", echo = FALSE, fig.width=7, fig.height=4}

#plot error rate wrt decreasing values of k
plot(x= 1/kvec, y=errors_ts, 
     type = "b", 
     xlim = c(0.01, 1), 
     ylim = c(0, 0.6), 
     log = "x", 
     xlab = "1/K",
     ylab = "Error Rate")
points(x = 1/k_star, y = errors_ts[k_star], col = "green", pch = 19)
text(x = 1/k_star, y = errors_ts[k_star], labels = paste("K = 9 and Error rate:", round(errors_ts[k_star], 2)), pos = 1)
```

# Predictions with the two models

We are now ready to take predictions on test set using both models. For the *k-NN*: given an observation, we will assign the majority class among k=9 neighbors. For *logistic regression*: given an observation, we will compute probabilities using the estimated coefficients and compare them with the threshold value of 0.5. If the estimated probability exceeds 0.5, the mother's breastfeeding behavior will be predicted as 1 (breastfeeding); otherwise, it will be classified as 0 (bottle).

```{r prediction}
#vector of predictions with k-nn where k = 9
knn_pred <- knn(x_train, x_test, y_tr, k = 9)
#vector of predictions with fitted logistic regression
mod1_predict <- predict(mod1, df_test, type= "response")
#classify using our 0.5 threshold:
glm_pred <- rep(0, nrow(df_test))
glm_pred[mod1_predict > 0.5] <- 1
```

Next, to assess performance evaluation, we utilize the confusion matrix and common evaluation metrics including **Accuracy** (proportion of correctly classified cases among all cases), **sensitivity** (proportion of true positives among all actual positive cases), **specificity** (proportion of true negatives among all actual negative cases) and **false positive rate** (proportion of negative cases mistakenly classified as positive among all actual negative cases).

# Performance evaluation, conclusion and limitation

Looking at Table 9 (a) and (b), both algorithms show similar numbers of correctly classified cases (sum of elements in the main diag), implying similar accuracy levels. However, Table 9 (c) reveals some differences: **k-NN model** achieves perfect sensitivity (1), indicating precise identification of all positive cases. However, its specificity is notably low at 0.22 (high false positive rate). This suggests difficulties in accurately classifying negative cases, as it tends to classify most instances into the majority class (positive). Conversely, despite having lower sensitivity (0.84), **Logistic regression model** outperforms k-NN in specificity with a score of 0.88, resulting in a markedly lower false positive rate (0.11) and demonstrating a superior classification ability.

```{r performance_evaluation, echo = FALSE}
#| label: tbl-mtcars
#| tbl-cap: "Performance evaluation"
#| tbl-subcap: ["k-NN (k=9) predictions compared to true breastfeeding behavior for 35 test set observations.", "Confusion matrix for logistic regression (threshold = 0.5) compared to true breastfeeding behavior for 35 test set observations.","Accurancy, sensitivity, specificity, false positive rate for both models"]
#| layout-ncol: 3

# confusion matrix for knn where k = 9
knn_conf<- table(knn_pred, y_ts)
row.names(knn_conf) <- c("pred 0","pred 1")
colnames(knn_conf) <- c("true 0","true 1")
# K-nn k=7 accurancy
k_acc<- mean(knn_pred == y_ts)
# K-nn k=7 sensitivity
k_sens <- sum(knn_pred ==1  & y_ts == 1) / sum(y_ts == 1)
# K-nn k=7 specificity
k_spec <- sum(knn_pred ==0  & y_ts == 0) / sum(y_ts == 0)
# K-nn k=7 false positive rate
k_fpr <- 1- k_spec

# confusion matrix for logist regression and T = 0.5 
log_conf <- table(glm_pred, y_ts)
row.names(log_conf) <- c("pred 0","pred 1")
colnames(log_conf) <- c("true 0","true 1")
# Logistic regression with T=0.5 accurancy
log_acc <- mean(glm_pred == y_ts)
# Logistic regression with T=0.5 sensitivity
log_sens <- sum(glm_pred ==1  & y_ts == 1) / sum(y_ts == 1)
# Logistic regression with T=0.5 specificity
log_spec <- sum(glm_pred ==0  & y_ts == 0) / sum(y_ts == 0)
# Logistic regression with T=0.5 precision
log_fpr <- 1- log_spec

# create row vectors
acc <- c(k_acc, log_acc)
sens <- c(k_sens, log_sens)
spec <- c(k_spec,log_spec)
fpr <- c(k_fpr,log_fpr)
# create a dataframe
df2<- rbind(acc,sens,spec,fpr)
#rename columns
colnames(df2) <- c("k-nn","log.model")
# convert to a table
summary_tbl <- as.table(df2)

#plot the tables
kable(knn_conf)
kable(log_conf)
kable(summary_tbl)
```

This observation is crucial for our study's objective of optimizing the targeting of breastfeeding promotions toward less inclined women. For instance, let's consider estimating the probability of a white, non-single, smoker mother (but not before), beginning her pregnancy, with friends who do not breastfeed, aged 24, and who discontinued schooling at 18, opting to breastfeed her baby despite not having been breastfed herself. We can make the predictions by using estimates for the regression coefficients from Table 8 as follows:

```{r predict_example}
#| tidy = TRUE
#probability estimation of the observation
predict_example <- predict(mod1, newdata = list(pregnancy="Beginning",
                                                howfed = "Bottle", 
                                                howfedfr = "Bottle", 
                                                partner="Partner", 
                                                smokenow="Yes", 
                                                smokebf="No",
                                                age = 24, 
                                                educat = 18, 
                                                ethnic="White" ), 
                           type = "response")

```

$$\hat{p}(1|X) = \frac{e^{-2.464 -0.002(\text{age=24}) + 0.183 (\text{educat}=18)  -1.662(\text{ethnic}=white) -3.212(\text{smokenow}=Yes)}}{1+e^{-2.464 -0.002(\text{age=24}) + 0.183 (\text{educat}=18)  -1.662(\text{ethnic}=white) -3.212(\text{smokenow}=Yes)}} = `r predict_example`$$ A potential limitation of this conclusion, and so of the study, lies in the sampling method utilized in section 2, impacting K-NN's performance due to dataset imbalance. However, techniques such as class weighting, batching, or resampling could alleviate this issue, enhancing overall model performance on imbalanced datasets.
