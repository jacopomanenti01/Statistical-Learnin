# Nested CV loop
tic()
for (i in 1:5){
leukemia_idx <- which(leukemia_folds == i)
leukemia_trn <- leukemia_sd[-leukemia_idx,]
leukemia_tst <- leukemia_sd[leukemia_idx,]
# save test labels for ROC CURVE plot
CV_list[[i]] <- leukemia_tst
# hard svm
hard_svm <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=10e5, scale=FALSE)
# soft smv
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
hyperpar[i,1]<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=hyperpar[i,1], scale=FALSE)
# radial svm
tune.radial <- tune(svm, y ~ ., data=leukemia_trn, kernel="radial",ranges=list(cost=cost_range, gamma=gamma_range))
hyperpar[i,2]<-  tune.radial$best.parameters$cost
hyperpar[i,3]<-  tune.radial$best.parameters$gamma
radial_svm <- svm(y ~ ., data=leukemia_trn, kernel="radial", gamma=hyperpar[i,3], cost=hyperpar[i,2])
# poly svm
tune.poly <- tune(svm, y ~ ., data = leukemia_trn, kernel = "poly", ranges = list(cost = cost_range, degree = poly_range ))
hyperpar[i,4]<-  tune.poly$best.parameters$cost
hyperpar[i,5]<-  tune.poly$best.parameters$degree
ply_svm <- svm(y ~ ., data=leukemia_trn, kernel="poly", cost=hyperpar[i,4], degree=hyperpar[i,5])
# evaluation time
print("hard results")
result1 <- evaluate_model(hard_svm, leukemia_tst)
predicted_cv[i,1] <- result1$err
ROC_list[[i]]$hard_svm <- result1$otp
print("soft results")
result2 <-  evaluate_model(soft_svm_out, leukemia_tst)
predicted_cv[i,2] <- result2$err
ROC_list[[i]]$sf_svm <- result2$otp
print("radial results")
result3 <- evaluate_model(radial_svm, leukemia_tst)
predicted_cv[i,3] <- result3$err
ROC_list[[i]]$radial_svm <- result3$otp
print("poly results")
result4 <- evaluate_model(ply_svm, leukemia_tst)
predicted_cv[i,4] <- result4$err
ROC_list[[i]]$poly_svm <- result4$otp
}
evaluate_model <- function(mod, x_ts) {
preds_ts <- predict(mod, x_ts, decision.values=TRUE)
cm_ts <- table(x_ts$y, preds_ts)
acc_ts <- mean(x_ts$y == preds_ts)
err_ts <- mean(x_ts$y != preds_ts)
print(cm_ts)
fitted.opt <- attributes(preds_ts)$decision.values
return(list(err = err_ts, otp = fitted.opt))
}
set.seed(1)
# split the data into 5 folds
leukemia_folds <-  createFolds(leukemia_sd$y, k = 5, list = FALSE)
# collect the predict score got from CV.
predicted_cv <-  matrix(0, nrow=5, ncol = 4,dimnames = list(NULL, c("hard svm", "soft svm", "radial svm", "poly smv")))
hyperpar <- matrix(0, nrow=5, ncol = 5,dimnames = list(NULL, c("soft svm", "cost radial ", "gamma radial smv", "cost poly", "gamma poly")))
CV_list <- list(1,2,3,4,5)
ROC_list <- lapply(CV_list, function(x) list("hard_svm" = NULL, "sf_svm" = NULL, "radial_svm" = NULL, "poly_svm" = NULL))
# setting the hyperparameters' value
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# Nested CV loop
tic()
for (i in 1:5){
leukemia_idx <- which(leukemia_folds == i)
leukemia_trn <- leukemia_sd[-leukemia_idx,]
leukemia_tst <- leukemia_sd[leukemia_idx,]
# save test labels for ROC CURVE plot
CV_list[[i]] <- leukemia_tst
# hard svm
hard_svm <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=10e5, scale=FALSE)
# soft smv
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
hyperpar[i,1]<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=hyperpar[i,1], scale=FALSE)
# radial svm
tune.radial <- tune(svm, y ~ ., data=leukemia_trn, kernel="radial",ranges=list(cost=cost_range, gamma=gamma_range))
hyperpar[i,2]<-  tune.radial$best.parameters$cost
hyperpar[i,3]<-  tune.radial$best.parameters$gamma
radial_svm <- svm(y ~ ., data=leukemia_trn, kernel="radial", gamma=hyperpar[i,3], cost=hyperpar[i,2])
# poly svm
tune.poly <- tune(svm, y ~ ., data = leukemia_trn, kernel = "poly", ranges = list(cost = cost_range, degree = poly_range ))
hyperpar[i,4]<-  tune.poly$best.parameters$cost
hyperpar[i,5]<-  tune.poly$best.parameters$degree
ply_svm <- svm(y ~ ., data=leukemia_trn, kernel="poly", cost=hyperpar[i,4], degree=hyperpar[i,5])
# evaluation time
print("hard results")
result1 <- evaluate_model(hard_svm, leukemia_tst)
predicted_cv[i,1] <- result1$err
ROC_list[[i]]$hard_svm <- result1$otp
print("soft results")
result2 <-  evaluate_model(soft_svm_out, leukemia_tst)
predicted_cv[i,2] <- result2$err
ROC_list[[i]]$sf_svm <- result2$otp
print("radial results")
result3 <- evaluate_model(radial_svm, leukemia_tst)
predicted_cv[i,3] <- result3$err
ROC_list[[i]]$radial_svm <- result3$otp
print("poly results")
result4 <- evaluate_model(ply_svm, leukemia_tst)
predicted_cv[i,4] <- result4$err
ROC_list[[i]]$poly_svm <- result4$otp
}
elapsed_time <- toc(log = TRUE)
print(elapsed_time$toc - elapsed_time$tic)
# Compute cv error
cv_error <- apply(predicted_cv, FUN = mean, MARGIN = 2)
cv_error
ROC_list
CV_list
CV_list[[1]]$y
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
rocplot(ROC_list[[1]]$hard_svm, CV_list[[1]]$y, main="Training Data")
rocplot(ROC_list[[1]]$sf_svm, CV_list[[1]]$y, add=T, col="red")
rocplot(ROC_list[[1]]$radial_svm, CV_list[[1]]$y, add=T, col="blue")
rocplot(ROC_list[[1]]$poly_svm, CV_list[[1]]$y, add=T, col="green")
abline(0, 1, col = "violet", lty = 1)
legend("topleft", legend=c("Hard svm", "Soft svm", "Radial svm", "poly svm", "random model"), col=c("black", "red", "blue", "green", "violet"),cex = 0.5 ,lty=1)
#| echo: FALSE
#| label: rocplot
rocplot <- function(pred, truth, ...){ # we use "..." to capture additional arguments to be passed to plot(), such as add=T
predob <- prediction(pred, truth)
perf <- performance(predob, "fpr","tpr")
plot(perf, ...)
}
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
rocplot(ROC_list[[1]]$hard_svm, CV_list[[1]]$y, main="Training Data")
rocplot(ROC_list[[1]]$sf_svm, CV_list[[1]]$y, add=T, col="red")
rocplot(ROC_list[[1]]$radial_svm, CV_list[[1]]$y, add=T, col="blue")
rocplot(ROC_list[[1]]$poly_svm, CV_list[[1]]$y, add=T, col="green")
abline(0, 1, col = "violet", lty = 1)
legend("topleft", legend=c("Hard svm", "Soft svm", "Radial svm", "poly svm", "random model"), col=c("black", "red", "blue", "green", "violet"),cex = 0.5 ,lty=1)
#| echo: FALSE
#| label: rocplot
rocplot <- function(pred, truth, ...){ # we use "..." to capture additional arguments to be passed to plot(), such as add=T
predob <- prediction(pred, truth)
perf <- performance(predob, "tpr", "fpr")
plot(perf, ...)
}
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
rocplot(ROC_list[[1]]$hard_svm, CV_list[[1]]$y, main="Training Data")
rocplot(ROC_list[[1]]$sf_svm, CV_list[[1]]$y, add=T, col="red")
rocplot(ROC_list[[1]]$radial_svm, CV_list[[1]]$y, add=T, col="blue")
rocplot(ROC_list[[1]]$poly_svm, CV_list[[1]]$y, add=T, col="green")
abline(0, 1, col = "violet", lty = 1)
legend("topleft", legend=c("Hard svm", "Soft svm", "Radial svm", "poly svm", "random model"), col=c("black", "red", "blue", "green", "violet"),cex = 0.5 ,lty=1)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE, , decision.values=TRUE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst, decision.values=TRUE)
fitted.opt <- attributes(yattr.opt)$decision.values
# ROC
rocplot(fitted.opt, leukemia_tst$y, main="Training Data")
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
?roc
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
install.packages("pROC")
install.packages("pROC")
library("pROC")
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
#| label: library
#| include: false
# Package names
packages <- c("tidyverse", "dplyr","ISLR2", "e1071","rsample", "caret","gridExtra","kableExtra","tictoc","ROCR")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
#| label: library
#| include: false
# Package names
packages <- c("tidyverse", "dplyr","ISLR2", "e1071","rsample", "caret","gridExtra","kableExtra","tictoc","ROCR","pROC")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
#| label: dataset
#| echo: false
# import data
leukemia <- read_tsv("gene_expr.tsv") # import data
# no missing data
print(paste("Number of na valuse: ",sum(is.na(leukemia))))
# Drop Sample.ID
leukemia <- leukemia %>%
select(-sampleID) %>%
mutate(y = factor(y))
# check the type
#column_types <- apply(leukemia, FUN = class, MARGIN=2 )
#print(paste("The dataset has",table(column_types)["numeric"][[1]], "characters columns type"))
# print dataset
head(leukemia, 4)
gene_sd<- apply(leukemia[, 1:2000], 2, sd )
# cut off 5%
cutoff <- quantile(gene_sd, 0.95)
# select top 5%
top_genes <- which(gene_sd >= cutoff)
top_sd <- gene_sd[top_genes]
# filter the dataset
leukemia_sd <- leukemia %>%
select(, c(top_genes, 2001))
d <- dim(leukemia_sd)
print(paste("After having filtered, the dataset has dimensions", d[1],d[2]))
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
yattr.opt
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
roc_svm_test
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
plot(roc_svm_test, add = TRUE,col = "red")
roc_svm_test
plot.new()
plot(roc_svm_test, add = TRUE,col = "red")
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
plot.new()
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor =as.numeric(yattr.opt))
plot.new()
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor = as.numeric(yattr.opt))
# Creazione del plot della ROC
plot(roc_svm_test, main = "ROC Curve for Soft SVM", col = "red")
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor = as.numeric(yattr.opt))
# Creazione del plot della ROC
plot(roc_svm_test, main = "ROC Curve for Soft SVM", col = "red", print.auc = TRUE)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# tune
tune.soft <- tune(svm, y ~ ., data=leukemia_trn, kernel="linear", ranges=list(cost=cost_range))
prova<-  tune.soft$best.parameters$cost
soft_svm_out <- svm(y ~ ., data=leukemia_trn, kernel="linear", cost=prova, scale=FALSE)
# pred
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor = as.numeric(yattr.opt))
# Creazione del plot della ROC
plot(roc_svm_test, main = "ROC Curve for Soft SVM", col = "red", print.auc = TRUE)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# Tuning del modello SVM
tune.soft <- tune(svm, y ~ ., data = leukemia_trn, kernel = "linear", ranges = list(cost = cost_range))
prova <- tune.soft$best.parameters$cost
# Addestramento del modello SVM con il miglior parametro
soft_svm_out <- svm(y ~ ., data = leukemia_trn, kernel = "linear", cost = prova, scale = FALSE)
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# Calcolo della ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor = as.numeric(yattr.opt))
# Creazione del plot della ROC con FPR sull'asse x
plot(roc_svm_test, main = "ROC Curve for Soft SVM", col = "red", print.auc = TRUE
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# Tuning del modello SVM
tune.soft <- tune(svm, y ~ ., data = leukemia_trn, kernel = "linear", ranges = list(cost = cost_range))
prova <- tune.soft$best.parameters$cost
# Addestramento del modello SVM con il miglior parametro
soft_svm_out <- svm(y ~ ., data = leukemia_trn, kernel = "linear", cost = prova, scale = FALSE)
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst)
# Calcolo della ROC
roc_svm_test <- roc(response = leukemia_tst$y, predictor = as.numeric(yattr.opt))
# Creazione del plot della ROC con FPR sull'asse x
plot(roc_svm_test, main = "ROC Curve for Soft SVM", col = "red", print.auc = TRUE)
?roc
?svm
# Addestramento del modello SVM con il miglior parametro
soft_svm_out <- svm(y ~ ., data = leukemia_trn, kernel = "linear", cost = prova, probability = TRUE)
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst, type="prob", probability = TRUE)
yattr.opt
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst, type="prob", probability = TRUE)
x.svm.prob.rocr <- prediction(attr(yattr.opt, "probabilities")[,2], leukemia_tst$y)
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# Tuning del modello SVM
tune.soft <- tune(svm, y ~ ., data = leukemia_trn, kernel = "linear", ranges = list(cost = cost_range))
prova <- tune.soft$best.parameters$cost
# Addestramento del modello SVM con il miglior parametro
soft_svm_out <- svm(y ~ ., data = leukemia_trn, kernel = "linear", cost = prova, probability = TRUE)
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst, type="prob", probability = TRUE)
# Calcolo della ROC
x.svm.prob.rocr <- prediction(attr(yattr.opt, "probabilities")[,2], leukemia_tst$y)
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
plot(x.svm.perf, col=6, add=TRUE)
#| echo: FALSE
#| label: roc
#| fig-cap: "The figure shows pairwise class comparison between the first 7 predictors. Clearly, the classes are not linearly separable."
set.seed(1)
# split the dataset into training and test set
train_size <- 0.7
my_split <- initial_split(leukemia_sd, strata = y, prop = train_size)
leukemia_trn <- training(my_split)
leukemia_tst <- testing(my_split)
# set
cost_range <- c(0.001, 0.01, 0.1, 1, 5, 10, 100)
gamma_range <- c(0.001, 0.01, 0.1, 0.5, 1, 3, 10)
poly_range <- c(1, 2, 3,4,5,6,10)
# Tuning del modello SVM
tune.soft <- tune(svm, y ~ ., data = leukemia_trn, kernel = "linear", ranges = list(cost = cost_range))
prova <- tune.soft$best.parameters$cost
# Addestramento del modello SVM con il miglior parametro
soft_svm_out <- svm(y ~ ., data = leukemia_trn, kernel = "linear", cost = prova, probability = TRUE)
# Predizione sui dati di test
yattr.opt <- predict(soft_svm_out, leukemia_tst, type="prob", probability = TRUE)
# Calcolo della ROC
x.svm.prob.rocr <- prediction(attr(yattr.opt, "probabilities")[,2], leukemia_tst$y)
x.svm.perf <- performance(x.svm.prob.rocr, "tpr","fpr")
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
plot.new()
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
plot(x.svm.perf, col=6, add=TRUE)
